{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarthCoder501/ML-AI-Projects/blob/main/Codebase%20RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessary Libraries"
      ],
      "metadata": {
        "id": "MpmkP4rM1KRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pygithub langchain langchain-community openai tiktoken pinecone-client langchain_pinecone sentence-transformers"
      ],
      "metadata": {
        "id": "BGFWnzpBDkWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAIXpUxWDFSV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "import tempfile\n",
        "from github import Github, Repository\n",
        "from git import Repo\n",
        "from openai import OpenAI\n",
        "from pathlib import Path\n",
        "from langchain.schema import Document\n",
        "from pinecone import Pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone a GitHub Repo locally"
      ],
      "metadata": {
        "id": "hTLsQ9Ma1FpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_repository(repo_url):\n",
        "    \"\"\"Clones a GitHub repository to a temporary directory.\n",
        "\n",
        "    Args:\n",
        "        repo_url: The URL of the GitHub repository.\n",
        "\n",
        "    Returns:\n",
        "        The path to the cloned repository.\n",
        "    \"\"\"\n",
        "    repo_name = repo_url.split(\"/\")[-1]  # Extract repository name from URL\n",
        "    repo_path = f\"/content/{repo_name}\"\n",
        "    Repo.clone_from(repo_url, str(repo_path))\n",
        "    return str(repo_path)"
      ],
      "metadata": {
        "id": "kKioMYZBDee4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = clone_repository(\"https://github.com/CoderAgent/SecureAgent\")"
      ],
      "metadata": {
        "id": "F_1zslPsDmJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(path)"
      ],
      "metadata": {
        "id": "hFrrr5rjEfYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUPPORTED_EXTENSIONS = {'.py', '.js', '.tsx', '.jsx', '.ipynb', '.java',\n",
        "                         '.cpp', '.ts', '.go', '.rs', '.vue', '.swift', '.c', '.h'}\n",
        "\n",
        "IGNORED_DIRS = {'node_modules', 'venv', 'env', 'dist', 'build', '.git',\n",
        "                '__pycache__', '.next', '.vscode', 'vendor'}"
      ],
      "metadata": {
        "id": "MQOcyi6DE5bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_content(file_path, repo_path):\n",
        "    \"\"\"\n",
        "    Get content of a single file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, str]]: Dictionary with file name and content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Get relative path from repo root\n",
        "        rel_path = os.path.relpath(file_path, repo_path)\n",
        "\n",
        "        return {\n",
        "            \"name\": rel_path,\n",
        "            \"content\": content\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_main_files_content(repo_path: str):\n",
        "    \"\"\"\n",
        "    Get content of supported code files from the local repository.\n",
        "\n",
        "    Args:\n",
        "        repo_path: Path to the local repository\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing file names and contents\n",
        "    \"\"\"\n",
        "    files_content = []\n",
        "\n",
        "    try:\n",
        "        for root, _, files in os.walk(repo_path):\n",
        "            # Skip if current directory is in ignored directories\n",
        "            if any(ignored_dir in root for ignored_dir in IGNORED_DIRS):\n",
        "                continue\n",
        "\n",
        "            # Process each file in current directory\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                if os.path.splitext(file)[1] in SUPPORTED_EXTENSIONS:\n",
        "                    file_content = get_file_content(file_path, repo_path)\n",
        "                    if file_content:\n",
        "                        files_content.append(file_content)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading repository: {str(e)}\")\n",
        "\n",
        "    return files_content"
      ],
      "metadata": {
        "id": "qi0FbfdrF6Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = get_main_files_content(path)"
      ],
      "metadata": {
        "id": "9-x-fTUqHISX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content"
      ],
      "metadata": {
        "id": "drDUCMcWIC-v",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "fTHEOUgp1Nmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)"
      ],
      "metadata": {
        "id": "pRz7UnvJoL-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am a programmer\"\n",
        "\n",
        "embeddings = get_huggingface_embeddings(text)"
      ],
      "metadata": {
        "id": "ojCvJduqIEQW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "6Oas_olkZSkU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Pinecone\n",
        "\n"
      ],
      "metadata": {
        "id": "umKbNfk3aBOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the PINECONE_API_KEY as an environment variable\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(\"codebase-rag\")"
      ],
      "metadata": {
        "id": "y05YK2IjaGgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())"
      ],
      "metadata": {
        "id": "OQN1SdEQbwDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "\n",
        "for file in file_content:\n",
        "    doc = Document(\n",
        "        page_content=f\"{file['name']}\\n{file['content']}\",\n",
        "        metadata={\"source\": file['name']}\n",
        "    )\n",
        "\n",
        "    documents.append(doc)\n",
        "\n",
        "\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    index_name=\"codebase-rag\",\n",
        "    namespace=\"https://github.com/CoderAgent/SecureAgent\"\n",
        ")"
      ],
      "metadata": {
        "id": "tDAB_siIb93B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform RAG\n"
      ],
      "metadata": {
        "id": "e75xrBVCrRL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "K9DJQMc_nrsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How are typescript files parsed?\""
      ],
      "metadata": {
        "id": "73ahsGAUnY22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "raw_query_embedding"
      ],
      "metadata": {
        "id": "Lwg3ZpsHnhhk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change the \"top_k\" parameter to be a higher or lower number\n",
        "top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=5, include_metadata=True, namespace=\"https://github.com/CoderAgent/SecureAgent\")"
      ],
      "metadata": {
        "id": "5DQKsXMunhkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_matches"
      ],
      "metadata": {
        "id": "_6JOvq6Tnhmf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts = [item['metadata']['text'] for item in top_matches['matches']]"
      ],
      "metadata": {
        "id": "ahEZbqcAnhox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts"
      ],
      "metadata": {
        "id": "VOwLKCRNqKF0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query"
      ],
      "metadata": {
        "id": "gHWR6szqqK7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(augmented_query)"
      ],
      "metadata": {
        "id": "AROG6MwJqNnC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"You are a Senior Software Engineer, specializing in TypeScript.\n",
        "\n",
        "Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response.\n",
        "\"\"\"\n",
        "\n",
        "llm_response = client.chat.completions.create(\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Q-l0gwYHqNpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "reEO69LpqNr9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together"
      ],
      "metadata": {
        "id": "OBi56NBnjYMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_rag(query):\n",
        "    raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "    top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=5, include_metadata=True, namespace=\"https://github.com/CoderAgent/SecureAgent\")\n",
        "\n",
        "    # Get the list of retrieved texts\n",
        "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "\n",
        "    # Modify the prompt below as need to improve the response quality\n",
        "    system_prompt = f\"\"\"You are a Senior Software Engineer, specializing in TypeScript.\n",
        "\n",
        "    Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response.\n",
        "    \"\"\"\n",
        "\n",
        "    llm_response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": augmented_query}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return llm_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "SUeLm0_uqkDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = perform_rag(\"How is the javascript parser used?\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "VWTmDGI-qkFm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buidling Web App"
      ],
      "metadata": {
        "id": "xGBV3wEiucAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok python-dotenv"
      ],
      "metadata": {
        "id": "7FwqRviPqkHy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n"
      ],
      "metadata": {
        "id": "L3d8frWXAKS6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "LDKAXsu1qkKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "ngrok.set_auth_token(ngrok_token)"
      ],
      "metadata": {
        "id": "3ukJjSr58ZnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_streamlit():\n",
        "  os.system('streamlit run /content/app.py --server.port 8501')"
      ],
      "metadata": {
        "id": "0LNkS2lc8bty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from rag import perform_rag  # Import the perform_rag function from your rag.py file\n",
        "\n",
        "# Set the Streamlit app configuration\n",
        "st.set_page_config(page_title=\"Codebase Chatbot\", layout=\"wide\")\n",
        "\n",
        "# Title and Description\n",
        "st.title(\"Codebase Chatbot\")\n",
        "st.write(\n",
        "    \"\"\"\n",
        "    Welcome to your codebase chatbot! Engage in a conversation about your codebase,\n",
        "    and receive detailed answers based on the embedded context.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Initialize session state for storing chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"Hi! I'm here to answer your questions about the codebase. How can I assist you today?\"}]  # List to hold chat messages\n",
        "\n",
        "# Display previous messages in the chat\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):  # `role` can be \"user\" or \"assistant\"\n",
        "        st.markdown(message[\"content\"])  # Render the message content\n",
        "\n",
        "# Input box for user query\n",
        "if user_input := st.chat_input(\"Ask your question about the codebase...\"):\n",
        "    # Add user message to session state\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_input)  # Display user's message\n",
        "\n",
        "    # Process the query with the RAG function\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Processing your query...\"):\n",
        "            try:\n",
        "                # Call perform_rag to generate a response\n",
        "                response = perform_rag(user_input)\n",
        "                st.markdown(response)  # Display assistant's response\n",
        "                # Add assistant's response to session state\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "            except Exception as e:\n",
        "                error_message = f\"An error occurred: {e}\"\n",
        "                st.error(error_message)\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_message})"
      ],
      "metadata": {
        "id": "K57VQ2kZl9lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag.py\n",
        "import requests\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set up API keys and configuration\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "PINECONE_ENVIRONMENT = 'us-east-1'\n",
        "PINECONE_INDEX_NAME = 'codebase-rag'\n",
        "PINECONE_NAMESPACE = 'https://github.com/CoderAgent/SecureAgent'\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "GROQ_MODEL = \"llama-3.1-70b-versatile\"\n",
        "\n",
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(\n",
        "    api_key= PINECONE_API_KEY,\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=PINECONE_ENVIRONMENT)\n",
        ")\n",
        "\n",
        "pinecone_index = pc.Index(PINECONE_INDEX_NAME)\n",
        "index_description = pc.describe_index(PINECONE_INDEX_NAME)\n",
        "print(index_description)\n",
        "def perform_rag(query):\n",
        "    \"\"\"\n",
        "    Perform Retrieval-Augmented Generation (RAG) to answer a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query.\n",
        "\n",
        "    Returns:\n",
        "        str: LLM-generated response.\n",
        "    \"\"\"\n",
        "    # Step 1: Generate embeddings for the query\n",
        "    raw_query_embedding = get_huggingface_embeddings(query)\n",
        "    raw_query_embedding_list = raw_query_embedding.tolist()\n",
        "\n",
        "    # Step 2: Query Pinecone for relevant contexts\n",
        "    top_matches = pinecone_index.query(\n",
        "        vector=raw_query_embedding_list,\n",
        "        top_k=5,\n",
        "        include_metadata=True,\n",
        "        namespace=PINECONE_NAMESPACE\n",
        "    )\n",
        "\n",
        "    # Extract contexts from matches\n",
        "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    # Step 3: Construct an augmented query\n",
        "    augmented_query = (\n",
        "        \"<CONTEXT>\\n\"\n",
        "        + \"\\n\\n-------\\n\\n\".join(contexts[:10])  # Use top 10 results\n",
        "        + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "    )\n",
        "\n",
        "    # Step 4: Define the system prompt\n",
        "    system_prompt = (\n",
        "        \"You are a Senior Software Engineer specializing in Python and JavaScript. \"\n",
        "        \"You are an expert in the Django framework.\\n\\n\"\n",
        "        \"Answer the following question about the codebase using the context provided. \"\n",
        "        \"Always explain your reasoning step by step.\"\n",
        "    )\n",
        "\n",
        "    # Step 5: Query the Llama 3.1 API\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    payload = {\n",
        "         \"model\": GROQ_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": augmented_query}\n",
        "        ],\n",
        "        \"max_tokens\": 1024,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", headers=headers, json=payload)\n",
        "    # Debugging Groq response\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        response_json = response.json()\n",
        "        try:\n",
        "            return response_json[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except (KeyError, IndexError):\n",
        "            return \"No response text found in the Groq API response.\"\n",
        "    else:\n",
        "        raise ValueError(f\"Error from Llama API: {response.status_code} - {response.text}\")"
      ],
      "metadata": {
        "id": "MbMeo12JmC__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread = Thread(target=run_streamlit)\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "MWmf4v7C93Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(addr='8501', proto='http', bind_tls=True)\n",
        "\n",
        "print(\"Public URL: \", public_url)"
      ],
      "metadata": {
        "id": "fauw2BX1945O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tunnels = ngrok.get_tunnels()\n",
        "for tunnel in tunnels:\n",
        "  print(f\"Closing tunnel: {tunnel.public_url} -> {tunnel.config['addr']}\")\n",
        "  ngrok.disconnect(tunnel.public_url)"
      ],
      "metadata": {
        "id": "5ajYeoaT98X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "\n",
        "GROQ_API_KEY = <Insert GROQ API key here>\n",
        "NGROK_AUTH_TOKEN = <Insert NGROK authentication token here>\n",
        "PINECONE_API_KEY = <Insert Pinecone API key here>"
      ],
      "metadata": {
        "id": "Br7Yz5O8vJvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}