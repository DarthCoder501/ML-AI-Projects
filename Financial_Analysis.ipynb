{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarthCoder501/ML-AI-Projects/blob/main/Financial_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ],
      "metadata": {
        "id": "p0ERkCmSSG-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install yfinance langchain_pinecone openai python-dotenv langchain-community sentence_transformers"
      ],
      "metadata": {
        "id": "jRh9rJEK2Eb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gskZos1j1eeV"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from openai import OpenAI\n",
        "import dotenv\n",
        "import json\n",
        "import yfinance as yf\n",
        "import concurrent.futures\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from langchain.schema import Document\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pinecone import Pinecone\n",
        "import numpy as np\n",
        "import requests\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stock_info(symbol: str) -> dict:\n",
        "    \"\"\"\n",
        "    Retrieves and formats detailed information about a stock from Yahoo Finance.\n",
        "\n",
        "    Args:\n",
        "        symbol (str): The stock ticker symbol to look up.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing detailed stock information, including ticker, name,\n",
        "              business summary, city, state, country, industry, and sector.\n",
        "    \"\"\"\n",
        "    data = yf.Ticker(symbol)\n",
        "    stock_info = data.info\n",
        "\n",
        "    properties = {\n",
        "        \"Ticker\": stock_info.get(\"symbol\", \"Information not available\"),\n",
        "        \"Name\": stock_info.get(\"longName\", \"Information not available\"),\n",
        "        \"Business Summary\": stock_info.get(\n",
        "            \"longBusinessSummary\", \"Information not available\"\n",
        "        ),\n",
        "        \"City\": stock_info.get(\"city\", \"Information not available\"),\n",
        "        \"State\": stock_info.get(\"state\", \"Information not available\"),\n",
        "        \"Country\": stock_info.get(\"country\", \"Information not available\"),\n",
        "        \"Industry\": stock_info.get(\"industry\", \"Information not available\"),\n",
        "        \"Sector\": stock_info.get(\"sector\", \"Information not available\"),\n",
        "    }\n",
        "\n",
        "    return properties"
      ],
      "metadata": {
        "id": "XkNRN6dRYg6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = yf.Ticker(\"NVDA\")\n",
        "stock_info = data.info\n",
        "print(stock_info)"
      ],
      "metadata": {
        "id": "pOuhq4-YTWvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    \"\"\"\n",
        "    Generates embeddings for the given text using a specified Hugging Face model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to generate embeddings for.\n",
        "        model_name (str): The name of the Hugging Face model to use.\n",
        "                          Defaults to \"sentence-transformers/all-mpnet-base-v2\".\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The generated embeddings as a NumPy array.\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)\n",
        "\n",
        "\n",
        "def cosine_similarity_between_sentences(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity between two sentences.\n",
        "\n",
        "    Args:\n",
        "        sentence1 (str): The first sentence for similarity comparison.\n",
        "        sentence2 (str): The second sentence for similarity comparison.\n",
        "\n",
        "    Returns:\n",
        "        float: The cosine similarity score between the two sentences,\n",
        "               ranging from -1 (completely opposite) to 1 (identical).\n",
        "\n",
        "    Notes:\n",
        "        Prints the similarity score to the console in a formatted string.\n",
        "    \"\"\"\n",
        "    # Get embeddings for both sentences\n",
        "    embedding1 = np.array(get_huggingface_embeddings(sentence1))\n",
        "    embedding2 = np.array(get_huggingface_embeddings(sentence2))\n",
        "\n",
        "    # Reshape embeddings for cosine_similarity function\n",
        "    embedding1 = embedding1.reshape(1, -1)\n",
        "    embedding2 = embedding2.reshape(1, -1)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(embedding1, embedding2)\n",
        "    similarity_score = similarity[0][0]\n",
        "    print(f\"Cosine similarity between the two sentences: {similarity_score:.4f}\")\n",
        "    return similarity_score\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"I like walking to the park\"\n",
        "sentence2 = \"I like running to the office\"\n",
        "\n",
        "similarity = cosine_similarity_between_sentences(sentence1, sentence2)"
      ],
      "metadata": {
        "id": "Zk1P2UGDLqsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aapl_info = get_stock_info('AAPL')\n",
        "print(aapl_info)"
      ],
      "metadata": {
        "id": "gxBgkHRH21Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aapl_description = aapl_info['Business Summary']\n",
        "\n",
        "company_description = \"I want to find companies that make smartphones and are headquarted in California\"\n",
        "\n",
        "similarity = cosine_similarity_between_sentences(aapl_description, company_description)"
      ],
      "metadata": {
        "id": "EGptiJJM23Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get all the Stocks in the Stock Market"
      ],
      "metadata": {
        "id": "3OoPZQWyRufp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to get the symbols (also known as tickers) of all the stocks in the stock market\n"
      ],
      "metadata": {
        "id": "HxbaF1X9R5od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_company_tickers():\n",
        "    \"\"\"\n",
        "    Downloads and parses the Stock ticker symbols from the GitHub-hosted SEC company tickers JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing company tickers and related information.\n",
        "\n",
        "    Notes:\n",
        "        The data is sourced from the official SEC website via a GitHub repository:\n",
        "        https://raw.githubusercontent.com/team-headstart/Financial-Analysis-and-Automation-with-LLMs/main/company_tickers.json\n",
        "    \"\"\"\n",
        "    # URL to fetch the raw JSON file from GitHub\n",
        "    url = \"https://raw.githubusercontent.com/team-headstart/Financial-Analysis-and-Automation-with-LLMs/main/company_tickers.json\"\n",
        "\n",
        "    # Making a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Checking if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the JSON content directly\n",
        "        company_tickers = json.loads(response.content.decode('utf-8'))\n",
        "\n",
        "        # Optionally save the content to a local file for future use\n",
        "        with open(\"company_tickers.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "            json.dump(company_tickers, file, indent=4)\n",
        "\n",
        "        print(\"File downloaded successfully and saved as 'company_tickers.json'\")\n",
        "        return company_tickers\n",
        "    else:\n",
        "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "company_tickers = get_company_tickers()"
      ],
      "metadata": {
        "id": "kfm8BFeuRs4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_tickers"
      ],
      "metadata": {
        "id": "BjAw0zuQSQxM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(company_tickers)"
      ],
      "metadata": {
        "id": "n2g0tXXISQz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "index_name = \"stocks\"\n",
        "namespace = \"stock-descriptions\"\n",
        "\n",
        "hf_embeddings = HuggingFaceEmbeddings()\n",
        "vectorstore = PineconeVectorStore(index_name=index_name, embedding=hf_embeddings)"
      ],
      "metadata": {
        "id": "4uYmYnT6SQ9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Processing\n",
        "\n",
        "It will take very long to process all the stocks like this!"
      ],
      "metadata": {
        "id": "943upXf3Fc5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n"
      ],
      "metadata": {
        "id": "1-0Pcf5uYN9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, stock in company_tickers.items():\n",
        "    stock_ticker = stock['ticker']\n",
        "\n",
        "    try:\n",
        "        stock_data = get_stock_info(stock_ticker)\n",
        "        stock_description = stock_data['Business Summary'] if stock_data['Business Summary'] != 'Information not available' else 'No description available'\n",
        "\n",
        "        print(f\"Processing stock {idx} / {len(company_tickers) - 1}: {stock_ticker}\",end=\"\\r\")\n",
        "\n",
        "        vectorstore_from_documents = PineconeVectorStore.from_documents(\n",
        "            documents=[Document(page_content=stock_description, metadata=stock_data)],\n",
        "            embedding=hf_embeddings,\n",
        "            index_name=index_name,\n",
        "            namespace=namespace\n",
        "        )\n",
        "\n",
        "        print(f\"Successfully processed stock {idx} / {len(company_tickers) - 1}: {stock_ticker}\")\n",
        "\n",
        "        with open(\"successful_tickers.txt\", \"a\") as success_file:\n",
        "            success_file.write(f\"{stock_ticker}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing stock {idx} / {len(company_tickers) - 1} ({stock_ticker}): {e}\")\n",
        "        with open(\"unsuccessful_tickers.txt\", \"a\") as error_file:\n",
        "            error_file.write(f\"{stock_ticker}\\n\")\n",
        "\n",
        "        if str(e) == \"can't start new thread\":\n",
        "            print(\"Stock processing failed due to thread limit. Terminating the process...\")\n",
        "            break\n",
        "\n",
        "    if int(idx) and int(idx) % 500 == 0:\n",
        "        print(\"Sleeping for 2 minutes to avoid rate limiting...\")\n",
        "        time.sleep(120)"
      ],
      "metadata": {
        "id": "C_B6b4WOYVYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parallelizing\n",
        "\n"
      ],
      "metadata": {
        "id": "g1POpkjJFqDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tracking lists\n",
        "successful_tickers = []\n",
        "unsuccessful_tickers = []\n",
        "\n",
        "# Load existing successful/unsuccessful tickers\n",
        "try:\n",
        "    with open('successful_tickers.txt', 'r') as f:\n",
        "        successful_tickers = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"Loaded {len(successful_tickers)} successful tickers\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No existing successful tickers file found\")\n",
        "\n",
        "try:\n",
        "    with open('unsuccessful_tickers.txt', 'r') as f:\n",
        "        unsuccessful_tickers = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"Loaded {len(unsuccessful_tickers)} unsuccessful tickers\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No existing unsuccessful tickers file found\")"
      ],
      "metadata": {
        "id": "xaUv_qnHhok8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_stock(stock_ticker: str) -> str:\n",
        "    # Skip if already processed\n",
        "    if stock_ticker in successful_tickers:\n",
        "        return f\"Already processed {stock_ticker}\"\n",
        "\n",
        "    try:\n",
        "        # Get and store stock data\n",
        "        stock_data = get_stock_info(stock_ticker)\n",
        "        stock_description = stock_data['Business Summary']\n",
        "\n",
        "        # Store stock description in Pinecone\n",
        "        vectorstore_from_texts = PineconeVectorStore.from_documents(\n",
        "            documents=[Document(page_content=stock_description, metadata=stock_data)],\n",
        "            embedding=hf_embeddings,\n",
        "            index_name=index_name,\n",
        "            namespace=namespace\n",
        "        )\n",
        "\n",
        "        # Track success\n",
        "        with open('successful_tickers.txt', 'a') as f:\n",
        "            f.write(f\"{stock_ticker}\\n\")\n",
        "        successful_tickers.append(stock_ticker)\n",
        "\n",
        "        return f\"Processed {stock_ticker} successfully\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Track failure\n",
        "        with open('unsuccessful_tickers.txt', 'a') as f:\n",
        "            f.write(f\"{stock_ticker}\\n\")\n",
        "        unsuccessful_tickers.append(stock_ticker)\n",
        "\n",
        "        return f\"ERROR processing {stock_ticker}: {e}\"\n",
        "\n",
        "def parallel_process_stocks(tickers: list, max_workers: int = 1) -> None:\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_ticker = {\n",
        "            executor.submit(process_stock, ticker): ticker\n",
        "            for ticker in tickers\n",
        "        }\n",
        "\n",
        "        for future in concurrent.futures.as_completed(future_to_ticker):\n",
        "            ticker = future_to_ticker[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                print(result)\n",
        "\n",
        "                # Stop on error\n",
        "                if result.startswith(\"ERROR\"):\n",
        "                    print(f\"Stopping program due to error in {ticker}\")\n",
        "                    executor.shutdown(wait=False)\n",
        "                    raise SystemExit(1)\n",
        "\n",
        "            except Exception as exc:\n",
        "                print(f'{ticker} generated an exception: {exc}')\n",
        "                print(\"Stopping program due to exception\")\n",
        "                executor.shutdown(wait=False)\n",
        "                raise SystemExit(1)\n",
        "\n",
        "# Prepare your tickers\n",
        "tickers_to_process = [company_tickers[num]['ticker'] for num in company_tickers.keys()]\n",
        "\n",
        "# Process them\n",
        "parallel_process_stocks(tickers_to_process, max_workers= 1)"
      ],
      "metadata": {
        "id": "mClWyH8HXD-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform RAG"
      ],
      "metadata": {
        "id": "HK_GCUV1OvIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "VPNLuCmQa0uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are some companies that manufacture consumer hardware?\""
      ],
      "metadata": {
        "id": "YkzNzCTdOrO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_query_embedding = get_huggingface_embeddings(query)"
      ],
      "metadata": {
        "id": "c2fO8ql_OrWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace)"
      ],
      "metadata": {
        "id": "nM2LkWyXPAiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_matches"
      ],
      "metadata": {
        "id": "wSV6Ic06QnEr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts = [item['metadata']['text'] for item in top_matches['matches']]"
      ],
      "metadata": {
        "id": "_U45gYqHPFdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query"
      ],
      "metadata": {
        "id": "eE1T_TAOPGAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(augmented_query)"
      ],
      "metadata": {
        "id": "BAnqmUbvPGCt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Groq for RAG\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s6oaODy2F0vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n"
      ],
      "metadata": {
        "id": "gCE-Z3eorbJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client = Groq(\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\"),\n",
        ")"
      ],
      "metadata": {
        "id": "BcnhtcGpP42e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"You are an expert at providing answers about stocks. Please answer my question provided.\n",
        "\"\"\"\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")\n",
        "response = chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "j_-UvIOmPGFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "e3usvOkOP1tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok python-dotenv"
      ],
      "metadata": {
        "id": "aJY0qoeOnkZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n"
      ],
      "metadata": {
        "id": "L_v0438mnkXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "jDIdRE_4nkUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "ngrok.set_auth_token(ngrok_token)"
      ],
      "metadata": {
        "id": "ebCuL4EtnkSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_streamlit():\n",
        "  os.system('streamlit run /content/app.py --server.port 8501')"
      ],
      "metadata": {
        "id": "6AXkYfv-nkP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from rag import perform_rag  # Import the perform_rag function from your rag.py file\n",
        "\n",
        "# Set the Streamlit app configuration\n",
        "st.set_page_config(page_title=\"Stock Research Assistant\", layout=\"wide\")\n",
        "\n",
        "# Title and Description\n",
        "st.title(\"Stock Chatbot\")\n",
        "st.write(\n",
        "    \"\"\"\n",
        "    Welcome to your Stocks chatbot! Engage in a conversation and receive answers based on your inputs.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Initialize session state for storing chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"Hi! I'm here to answer your questions about stocks. How can I assist you today?\"}]  # List to hold chat messages\n",
        "\n",
        "# Display previous messages in the chat\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):  # `role` can be \"user\" or \"assistant\"\n",
        "        st.markdown(message[\"content\"])  # Render the message content\n",
        "\n",
        "# Input box for user query\n",
        "if user_input := st.chat_input(\"Ask your question about a stock...\"):\n",
        "    # Add user message to session state\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(user_input)  # Display user's message\n",
        "\n",
        "    # Process the query with the RAG function\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Processing your query...\"):\n",
        "            try:\n",
        "                # Call perform_rag to generate a response\n",
        "                response = perform_rag(user_input)\n",
        "                st.markdown(response)  # Display assistant's response\n",
        "                # Add assistant's response to session state\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "            except Exception as e:\n",
        "                error_message = f\"An error occurred: {e}\"\n",
        "                st.error(error_message)\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_message})"
      ],
      "metadata": {
        "id": "OqkHHfVDnkNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag.py\n",
        "import requests\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set up API keys and configuration\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "PINECONE_ENVIRONMENT = 'us-east-1'\n",
        "PINECONE_INDEX_NAME = 'stocks'\n",
        "PINECONE_NAMESPACE = 'stock-descriptions'\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "GROQ_MODEL = \"llama-3.1-70b-versatile\"\n",
        "\n",
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(\n",
        "    api_key= PINECONE_API_KEY,\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=PINECONE_ENVIRONMENT)\n",
        ")\n",
        "\n",
        "pinecone_index = pc.Index(PINECONE_INDEX_NAME)\n",
        "index_description = pc.describe_index(PINECONE_INDEX_NAME)\n",
        "print(index_description)\n",
        "def perform_rag(query):\n",
        "    \"\"\"\n",
        "    Perform Retrieval-Augmented Generation (RAG) to answer a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query.\n",
        "\n",
        "    Returns:\n",
        "        str: LLM-generated response.\n",
        "    \"\"\"\n",
        "    # Step 1: Generate embeddings for the query\n",
        "    raw_query_embedding = get_huggingface_embeddings(query)\n",
        "    raw_query_embedding_list = raw_query_embedding.tolist()\n",
        "\n",
        "    # Step 2: Query Pinecone for relevant contexts\n",
        "    top_matches = pinecone_index.query(\n",
        "        vector=raw_query_embedding_list,\n",
        "        top_k=5,\n",
        "        include_metadata=True,\n",
        "        namespace=PINECONE_NAMESPACE\n",
        "    )\n",
        "\n",
        "    # Extract contexts from matches\n",
        "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    # Step 3: Construct an augmented query\n",
        "    augmented_query = (\n",
        "        \"<CONTEXT>\\n\"\n",
        "        + \"\\n\\n-------\\n\\n\".join(contexts[:10])  # Use top 10 results\n",
        "        + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "    )\n",
        "\n",
        "    # Step 4: Define the system prompt\n",
        "    system_prompt = (\n",
        "        \"You are an expert at providing answers about stocks. Please answer my question provided.\"\n",
        "        \"Always explain your reasoning step by step.\"\n",
        "    )\n",
        "\n",
        "    # Step 5: Query the Llama 3.1 API\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    payload = {\n",
        "         \"model\": GROQ_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": augmented_query}\n",
        "        ],\n",
        "        \"max_tokens\": 1024,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", headers=headers, json=payload)\n",
        "    # Debugging Groq response\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        response_json = response.json()\n",
        "        try:\n",
        "            return response_json[\"choices\"][0][\"message\"][\"content\"]\n",
        "        except (KeyError, IndexError):\n",
        "            return \"No response text found in the Groq API response.\"\n",
        "    else:\n",
        "        raise ValueError(f\"Error from Llama API: {response.status_code} - {response.text}\")"
      ],
      "metadata": {
        "id": "oxcEmRCknkLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile emeddings.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)"
      ],
      "metadata": {
        "id": "uztuUHT-pOLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread = Thread(target=run_streamlit)\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "g752Zb1ankIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(addr='8501', proto='http', bind_tls=True)\n",
        "\n",
        "print(\"Public URL: \", public_url)"
      ],
      "metadata": {
        "id": "90dbKRCTnkGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tunnels = ngrok.get_tunnels()\n",
        "for tunnel in tunnels:\n",
        "  print(f\"Closing tunnel: {tunnel.public_url} -> {tunnel.config['addr']}\")\n",
        "  ngrok.disconnect(tunnel.public_url)"
      ],
      "metadata": {
        "id": "DfV8nHm0pa82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "\n",
        "GROQ_API_KEY =\n",
        "NGROK_AUTH_TOKEN =\n",
        "PINECONE_API_KEY ="
      ],
      "metadata": {
        "id": "xQj6qO3KP1v5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}